{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Flatten, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from models.model import DARKNET19_ARCHITECTURE, INPUT_SIZE, build_model\n",
    "from keras.utils import np_utils\n",
    "import albumentations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "WIDTH = 224\n",
    "HEIGHT = 224 \n",
    "BATCH_SIZE = 32\n",
    "TEST_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "with open(\"bboxes.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get data and transform left hand labels to 0 and right hand labels to 1\n",
    "df = pd.DataFrame.from_dict(data).T\n",
    "label_names = ['palmar left', 'palmar right']\n",
    "df.replace(label_names , [0, 1], inplace=True)\n",
    "\n",
    "# # Count the number of left and right hands in the data\n",
    "# labels = df[\"label\"].value_counts()\n",
    "# print(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(df, test_size=TEST_SPLIT)\n",
    "\n",
    "\n",
    "train_images, train_bboxes, train_labels = train.index.to_numpy(), train[\"bbox\"].to_numpy(), train[\"label\"].to_numpy()\n",
    "test_images, test_bboxes, test_labels = test.index.to_numpy(), test[\"bbox\"].to_numpy(), test[\"label\"].to_numpy()\n",
    "\n",
    "train_labels = np.array([[x] for x in train_labels], dtype=np.float32)\n",
    "test_labels = np.array([[x] for x in test_labels], dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "### 2.1 Resizing & Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, bbox, label):\n",
    "    \n",
    "    filename = path.decode(\"utf-8\")\n",
    "    path = f\"data/{filename}\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    transform = albumentations.Compose(\n",
    "        [albumentations.Resize(height=HEIGHT, width=WIDTH, always_apply=True)],\n",
    "        bbox_params=albumentations.BboxParams(format='coco'))\n",
    "\n",
    "    transformed = transform(image=image, bboxes=[np.concatenate([bbox,[0]])])\n",
    "    image, bbox = transformed[\"image\"], transformed[\"bboxes\"][0]\n",
    "\n",
    "    # cv2.circle(image,(int(bbox[0]),int(bbox[1])), 2, (0,255,0), -1)\n",
    "    # cv2.circle(image,(int(bbox[0])+int(bbox[2]),int(bbox[1])+int(bbox[3])), 2, (0,255,0), -1)\n",
    "    # plt.imshow(image)\n",
    "    # plt.show()\n",
    "    \n",
    "    norm_image = tf.cast(image, tf.float32) / 255.\n",
    "\n",
    "    x1, y1, x2, y2 = bbox[0]/WIDTH, bbox[1]/HEIGHT, bbox[2]/WIDTH, bbox[3]/HEIGHT\n",
    "    norm_bbox = np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "\n",
    "\n",
    "    return norm_image, norm_bbox, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transforming data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(image, bbox, label):\n",
    "    image, bbox, label = tf.numpy_function(read_image, [image, bbox, label], [tf.float32, tf.float32, tf.float32])\n",
    "    image.set_shape((WIDTH, HEIGHT, 3))\n",
    "    bbox.set_shape((4))\n",
    "    label.set_shape((1))\n",
    "    return image, (bbox, label)\n",
    "\n",
    "# print(train_labels)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, list(train_bboxes), train_labels))\n",
    "train_dataset = train_dataset.map(parse).batch(5)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, list(test_bboxes), test_labels))\n",
    "test_dataset = test_dataset.map(parse).batch(5)\n",
    "\n",
    "# for X, [bbox, label]  in train_dataset:\n",
    "#     print(X.shape, bbox.shape, label.shape)\n",
    "#     break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(DARKNET19_ARCHITECTURE, INPUT_SIZE)\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss={\n",
    "    \"label\": \"binary_crossentropy\",\n",
    "    \"bbox\": \"mean_squared_error\"\n",
    "},\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"models\", verbose=1, save_best_only=True),\n",
    "    CSVLogger(\"models\", append=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "]\n",
    "\n",
    "print(train_dataset)\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
